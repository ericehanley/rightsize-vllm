{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How much VRAM does my model need for my specific use case?\n",
        "\n",
        "This notebook estimates the **total VRAM required** to run inference for your specific use case. To get a tailored calculation, you can specify:\n",
        "\n",
        "* A model name from the Hugging Face Hub.\n",
        "\n",
        "* Your Hugging Face API key (if using a gated model).\n",
        "\n",
        "* The number of active parameters in billions.\n",
        "\n",
        "* The average input and output length (in tokens) for your workload.\n",
        "\n",
        "* The number of concurrent users you want to support.\n",
        "\n",
        "The final calculated memory requirement will help you select the right hardware - and this can be used for both GPU and TPU selection. For context, here are the memory capacities of some common accelerator types:\n",
        "\n",
        "```\n",
        "1 L4 GPU = 24 GB\n",
        "1 H100 GPU = 80 GB\n",
        "1-chip v5e TPU (ct5lp-hightpu-1t) = 16 GB\n",
        "8-chip v5e TPU (ct5lp-hightpu-8t) = 128 GB\n",
        "A g2-standard-96 machine with 8 L4 GPUs = 192 GB\n",
        "An a3-highgpu-8g machine with 8 H100 GPUs = 640 GB\n",
        "\n",
        "```\n",
        "\n",
        "## What the Notebook Calculates\n",
        "The notebook breaks down the memory requirement into several key components before providing a final, actionable number:\n",
        "\n",
        "* Estimated Model Weight: The memory needed to load the model's parameters, based on a common estimation formula derived from the model's configuration.\n",
        "\n",
        "* Activation Memory: The temporary \"scratchpad\" memory used by the GPU during the model's computation for a single request.\n",
        "\n",
        "* KV Cache Memory: The memory required to store the attention context (Key-Value cache) for each concurrent user, which grows with the sequence length.\n",
        "\n",
        "* ✅ Total Required GPU Memory: The final estimated VRAM your configuration will need. This combines the model weight, overhead, activations, and the KV cache for all concurrent users, adjusted for your desired memory utilization.\n",
        "\n",
        "*Note: Multi-GPU cases might incur additional memory usage for communication overhead, depending on how the model and KV cache are distributed across devices.*"
      ],
      "metadata": {
        "id": "-TofAGpcW05p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkChG5g9R4CR",
        "outputId": "a10b6d05-7ab4-4cd3-f091-904f07f78a5c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using nested 'text_config' for model parameters.\n",
            "\n",
            "--- Calculating Required GPU Memory ---\n",
            "1. Model Weight: 54.00 GB\n",
            "   (Based on user input of 27B parameters)\n",
            "2. Non-PyTorch Memory (Overhead): 1.00 GB\n",
            "3. PyTorch Activation Peak Memory (per request): 0.31 GB\n",
            "4. KV Cache Memory (per request): 1.13 GB\n",
            "\n",
            "--- Total Memory Calculation ---\n",
            "Total Memory for 10 users (unadjusted): 66.64 GB\n",
            "\n",
            "-------------------------------------\n",
            "✅ Required GPU Memory: 66.64 GB\n",
            "-------------------------------------\n",
            "\n",
            "This is the estimated total GPU VRAM needed to serve 10 concurrent users with the specified model and sequence lengths.\n"
          ]
        }
      ],
      "source": [
        "# @title Resource requirements for LLM inference\n",
        "huggingface_api_key = \"\" # @param {type:\"string\"}\n",
        "huggingface_model_name = \"google/gemma-3-27b-it\" # @param {type:\"string\"}\n",
        "model_parameters_in_billions = 27 # @param {type:\"number\"}\n",
        "avg_input_length = 1500 # @param {type:\"integer\"}\n",
        "avg_output_length = 200 # @param {type:\"integer\"}\n",
        "concurrent_users = 10 # @param {type:\"integer\"}\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Create a directory to store the config files\n",
        "os.makedirs(\"config_files\", exist_ok=True)\n",
        "\n",
        "# Download the model's config.json file\n",
        "config_url = f\"https://huggingface.co/{huggingface_model_name}/resolve/main/config.json?download=true\"\n",
        "headers = {\"Authorization\": f\"Bearer {huggingface_api_key}\"}\n",
        "response = requests.get(config_url, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(os.path.join(\"config_files\", f\"config.json\"), \"w\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "with open(os.path.join(\"config_files\", f\"config.json\"), \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# Check for a nested text_config, common in multimodal models.\n",
        "if \"text_config\" in data and isinstance(data[\"text_config\"], dict):\n",
        "    config_source = data[\"text_config\"]\n",
        "    print(\"Using nested 'text_config' for model parameters.\")\n",
        "else:\n",
        "    config_source = data\n",
        "    print(\"Using top-level config for model parameters.\")\n",
        "\n",
        "# Use .get() for safe dictionary access\n",
        "hidden_size = config_source.get('hidden_size')\n",
        "num_hidden_layers = config_source.get('num_hidden_layers')\n",
        "num_attention_heads = config_source.get('num_attention_heads')\n",
        "intermediate_size = config_source.get('intermediate_size')\n",
        "num_kv_heads = config_source.get('num_key_value_heads', num_attention_heads)\n",
        "\n",
        "# Ensure all required parameters were found\n",
        "required_params = [hidden_size, num_hidden_layers, num_attention_heads, intermediate_size]\n",
        "if not all(required_params):\n",
        "    raise ValueError(\"One or more required model parameters (e.g., hidden_size) could not be found in the config.\")\n",
        "\n",
        "head_dims = hidden_size // num_attention_heads\n",
        "dtype = data.get('torch_dtype', 'bfloat16')\n",
        "\n",
        "# Determine data type size in bytes\n",
        "match dtype:\n",
        "  case 'float16' | 'bfloat16':\n",
        "    parameter_data_type_size = 2\n",
        "    kv_data_type_size = 2\n",
        "  case 'float32':\n",
        "    parameter_data_type_size = 4\n",
        "    kv_data_type_size = 4\n",
        "  case _:\n",
        "    parameter_data_type_size = 2\n",
        "    kv_data_type_size = 2\n",
        "\n",
        "\n",
        "# @title Calculate Required GPU Memory\n",
        "print(\"\\n--- Calculating Required GPU Memory ---\")\n",
        "\n",
        "# --- Component 1: Model Weight (from User Input) ---\n",
        "# The number of parameters is provided directly by the user.\n",
        "number_of_model_parameters = model_parameters_in_billions * 1e9\n",
        "model_weight_bytes = number_of_model_parameters * parameter_data_type_size\n",
        "model_weight_gb = model_weight_bytes / (1000**3)\n",
        "\n",
        "print(f\"1. Model Weight: {model_weight_gb:.2f} GB\")\n",
        "print(f\"   (Based on user input of {model_parameters_in_billions}B parameters)\")\n",
        "\n",
        "# --- Component 2: Non-PyTorch Memory ---\n",
        "non_torch_memory_gb = 1.0\n",
        "print(f\"2. Non-PyTorch Memory (Overhead): {non_torch_memory_gb:.2f} GB\")\n",
        "\n",
        "# --- Component 3: PyTorch Activation Peak Memory ---\n",
        "sequence_length = avg_input_length + avg_output_length\n",
        "pytorch_activation_peak_memory_bytes = sequence_length * (18 * hidden_size + 4 * intermediate_size)\n",
        "pytorch_activation_peak_memory_gb = pytorch_activation_peak_memory_bytes / (1000**3)\n",
        "print(f\"3. PyTorch Activation Peak Memory (per request): {pytorch_activation_peak_memory_gb:.2f} GB\")\n",
        "\n",
        "# --- Component 4: KV Cache Memory ---\n",
        "kv_vectors = 2\n",
        "kv_cache_memory_per_batch_bytes = (kv_vectors * num_kv_heads * head_dims * num_hidden_layers * kv_data_type_size) * sequence_length\n",
        "kv_cache_memory_per_batch_gb = kv_cache_memory_per_batch_bytes / (1000**3)\n",
        "print(f\"4. KV Cache Memory (per request): {kv_cache_memory_per_batch_gb:.2f} GB\")\n",
        "\n",
        "print(\"\\n--- Total Memory Calculation ---\")\n",
        "\n",
        "# --- Final Calculation ---\n",
        "static_memory_gb = model_weight_gb + non_torch_memory_gb + pytorch_activation_peak_memory_gb\n",
        "total_kv_cache_for_all_users_gb = kv_cache_memory_per_batch_gb * concurrent_users\n",
        "total_unadjusted_memory_gb = static_memory_gb + total_kv_cache_for_all_users_gb\n",
        "required_gpu_memory_gb = total_unadjusted_memory_gb\n",
        "\n",
        "print(f\"Total Memory for {concurrent_users} users (unadjusted): {total_unadjusted_memory_gb:.2f} GB\")\n",
        "\n",
        "print(\"\\n-------------------------------------\")\n",
        "print(f\"✅ Required GPU Memory: {required_gpu_memory_gb:.2f} GB\")\n",
        "print(\"-------------------------------------\")\n",
        "print(f\"\\nThis is the estimated total GPU VRAM needed to serve {concurrent_users} concurrent users with the specified model and sequence lengths.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f"
      ],
      "metadata": {
        "id": "VUrWz-I9BO4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}