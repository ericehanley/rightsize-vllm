{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙️ How Much VRAM Does My Model Need?\n",
        "\n",
        "This notebook estimates the **total VRAM required** to run inference for your specific use case. To get a tailored calculation, you'll specify:\n",
        "\n",
        "* A model from the Hugging Face Hub.\n",
        "* Your Hugging Face API key (for gated models).\n",
        "* The number of model parameters (in billions).\n",
        "* The average input and output length (in tokens).\n",
        "* The **batch size** you plan to use for inference.\n",
        "  * NOTE that a batch size of one will provide you with the minimum required GPU/TPU memory required for your workload.\n",
        "\n",
        "The final calculation will help you select the right GPU or TPU hardware. For context, here are the memory capacities of some common accelerator types:\n",
        "\n",
        "```text\n",
        "1 L4 GPU          = 24 GB\n",
        "1 H100 GPU        = 80 GB\n",
        "1-chip v5e TPU    = 16 GB\n",
        "8-chip v5e TPU    = 128 GB\n",
        "8x L4 GPUs        = 192 GB\n",
        "8x H100 GPUs      = 640 GB\n",
        "```\n",
        "\n",
        "---  \n",
        "\n",
        "\n",
        "## What This Notebook Calculates\n",
        "\n",
        "The notebook breaks down the VRAM requirement into several key components before providing a final, actionable number:\n",
        "\n",
        "* **Model Weight Memory**: The memory needed to load the model's parameters onto the accelerator.\n",
        "* **Activation Memory**: The temporary \"scratchpad\" memory used during computation. This scales directly with your batch size and sequence length.\n",
        "* **KV Cache Memory**: The memory required to store the attention context (Key-Value cache) for the entire batch, which is crucial for efficient token generation.\n",
        "* **✅ Total Required Memory**: The final estimated VRAM your configuration will need. This is the sum of the model weight, system overhead, activation memory, and KV cache for your specified batch size.\n",
        "\n",
        "*Note: Multi-GPU/TPU setups might incur additional memory usage for communication overhead, depending on how the model and data are distributed across devices.*"
      ],
      "metadata": {
        "id": "-TofAGpcW05p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tkChG5g9R4CR",
        "outputId": "506e89f3-36f0-4843-b2b9-547fdfa6d1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fetching configuration for google/gemma-3-27b-it ---\n",
            "Using nested 'text_config' for model parameters.\n",
            "\n",
            "--- Calculating Required GPU/TPU Memory ---\n",
            "1. Model Weight Memory: 54.00 GB\n",
            "2. System Overhead Memory: 1.00 GB\n",
            "3. PyTorch Activation Memory (for batch size 1): 0.31 GB\n",
            "4. KV Cache Memory (for batch size 1): 1.13 GB\n",
            "\n",
            "--- Total Memory Calculation ---\n",
            "\n",
            "-------------------------------------\n",
            "✅ Required GPU/TPU Memory: 56.44 GB\n",
            "-------------------------------------\n",
            "\n",
            "This is the estimated total GPU/TPU VRAM needed to run inference for a batch size of 1 with the specified model and sequence lengths.\n"
          ]
        }
      ],
      "source": [
        "# @title Resource requirements for LLM inference\n",
        "huggingface_api_key = \"\" # @param {type:\"string\"}\n",
        "huggingface_model_name = \"google/gemma-3-27b-it\" # @param {type:\"string\"}\n",
        "model_parameters_in_billions = 27 # @param {type:\"number\"}\n",
        "avg_input_length = 1500 # @param {type:\"integer\"}\n",
        "avg_output_length = 200 # @param {type:\"integer\"}\n",
        "batch_size = 1 # @param {type:\"integer\"}\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# --- Step 1: Fetch Model Configuration ---\n",
        "print(f\"--- Fetching configuration for {huggingface_model_name} ---\")\n",
        "\n",
        "# Create a directory to store the config files\n",
        "os.makedirs(\"config_files\", exist_ok=True)\n",
        "config_path = os.path.join(\"config_files\", \"config.json\")\n",
        "\n",
        "# Download the model's config.json file\n",
        "config_url = f\"https://huggingface.co/{huggingface_model_name}/resolve/main/config.json?download=true\"\n",
        "headers = {\"Authorization\": f\"Bearer {huggingface_api_key}\"}\n",
        "response = requests.get(config_url, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# Check for a nested text_config, common in multimodal models.\n",
        "if \"text_config\" in data and isinstance(data[\"text_config\"], dict):\n",
        "    config_source = data[\"text_config\"]\n",
        "    print(\"Using nested 'text_config' for model parameters.\")\n",
        "else:\n",
        "    config_source = data\n",
        "    print(\"Using top-level config for model parameters.\")\n",
        "\n",
        "# Use .get() for safe dictionary access to extract model architecture details\n",
        "hidden_size = config_source.get('hidden_size')\n",
        "num_hidden_layers = config_source.get('num_hidden_layers')\n",
        "num_attention_heads = config_source.get('num_attention_heads')\n",
        "intermediate_size = config_source.get('intermediate_size')\n",
        "num_kv_heads = config_source.get('num_key_value_heads', num_attention_heads)\n",
        "\n",
        "# Ensure all required parameters were found\n",
        "required_params = [hidden_size, num_hidden_layers, num_attention_heads, intermediate_size]\n",
        "if not all(required_params):\n",
        "    raise ValueError(\"One or more required model parameters (e.g., hidden_size, num_hidden_layers) could not be found in the config.\")\n",
        "\n",
        "head_dims = hidden_size // num_attention_heads\n",
        "dtype = data.get('torch_dtype', 'bfloat16')\n",
        "\n",
        "# Determine data type size in bytes based on model's precision (dtype)\n",
        "match dtype:\n",
        "  case 'float16' | 'bfloat16':\n",
        "    parameter_data_type_size = 2\n",
        "    kv_data_type_size = 2\n",
        "  case 'float32':\n",
        "    parameter_data_type_size = 4\n",
        "    kv_data_type_size = 4\n",
        "  case _: # Default to bfloat16 if not specified\n",
        "    parameter_data_type_size = 2\n",
        "    kv_data_type_size = 2\n",
        "\n",
        "# --- Step 2: Calculate Required GPU/TPU Memory ---\n",
        "print(\"\\n--- Calculating Required GPU/TPU Memory ---\")\n",
        "\n",
        "# Component 1: Model Weight Memory (Static)\n",
        "# Memory needed to load the model's parameters onto the GPU.\n",
        "number_of_model_parameters = model_parameters_in_billions * 1e9\n",
        "model_weight_bytes = number_of_model_parameters * parameter_data_type_size\n",
        "model_weight_gb = model_weight_bytes / (1000**3)\n",
        "print(f\"1. Model Weight Memory: {model_weight_gb:.2f} GB\")\n",
        "\n",
        "# Component 2: Overhead Memory (Static)\n",
        "# Fixed memory for non-PyTorch components like CUDA kernels, etc.\n",
        "overhead_memory_gb = 1.0\n",
        "print(f\"2. System Overhead Memory: {overhead_memory_gb:.2f} GB\")\n",
        "\n",
        "# Component 3: PyTorch Activation Peak Memory (Dynamic)\n",
        "# Memory for storing intermediate calculations (activations) during the forward pass.\n",
        "# This scales with batch size and sequence length.\n",
        "sequence_length = avg_input_length + avg_output_length\n",
        "pytorch_activation_peak_memory_bytes = batch_size * sequence_length * (18 * hidden_size + 4 * intermediate_size)\n",
        "pytorch_activation_peak_memory_gb = pytorch_activation_peak_memory_bytes / (1000**3)\n",
        "print(f\"3. PyTorch Activation Memory (for batch size {batch_size}): {pytorch_activation_peak_memory_gb:.2f} GB\")\n",
        "\n",
        "# Component 4: KV Cache Memory (Dynamic)\n",
        "# Memory for the Key-Value cache, which stores attention context to speed up token generation.\n",
        "# This scales with batch size and sequence length.\n",
        "kv_vectors = 2 # One for Key, one for Value\n",
        "kv_cache_memory_per_batch_bytes = kv_vectors * batch_size * sequence_length * num_kv_heads * head_dims * num_hidden_layers * kv_data_type_size\n",
        "kv_cache_memory_per_batch_gb = kv_cache_memory_per_batch_bytes / (1000**3)\n",
        "print(f\"4. KV Cache Memory (for batch size {batch_size}): {kv_cache_memory_per_batch_gb:.2f} GB\")\n",
        "\n",
        "# --- Final Calculation ---\n",
        "print(\"\\n--- Total Memory Calculation ---\")\n",
        "# Sum of static and dynamic memory components.\n",
        "required_gpu_memory_gb = (\n",
        "    model_weight_gb +\n",
        "    overhead_memory_gb +\n",
        "    pytorch_activation_peak_memory_gb +\n",
        "    kv_cache_memory_per_batch_gb\n",
        ")\n",
        "\n",
        "print(\"\\n-------------------------------------\")\n",
        "print(f\"✅ Required GPU/TPU Memory: {required_gpu_memory_gb:.2f} GB\")\n",
        "print(\"-------------------------------------\")\n",
        "print(f\"\\nThis is the estimated total GPU/TPU VRAM needed to run inference for a batch size of {batch_size} with the specified model and sequence lengths.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tYZbPvbD6vvs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
